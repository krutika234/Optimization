{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Unconstarint optimization.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZgTBMr2hoW",
        "colab_type": "text"
      },
      "source": [
        "#### Unconstarint optimization \n",
        "####  Objective function \n",
        "\n",
        "#### Minimize: $f(x,y) = (1-x) $<sup> 2 </sup> $+ 100*(y - x$<sup>2</sup>) <sup> 2</sup>\n",
        "    \n",
        "    \n",
        "#### function is minimum at (1,1) with the value zero.\n",
        "##### The first step to start with is to either declare the rosenback function or to simply call the inbuilt rosenback function from scipy library \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgDvHr_M1le6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# installing the scipy library \n",
        "!pip install scipy  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzKVQKuI1lfC",
        "colab_type": "text"
      },
      "source": [
        "#### The above command will help you to install the scipy in python\n",
        "#### Importing the  required libraries "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YPd09K51lfE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import optimize as so\n",
        "import numpy as np\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Et4NSs61lfJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HNAXWr1lfL",
        "colab_type": "code",
        "colab": {},
        "outputId": "67a231fd-aa93-422a-9bf2-542f11183c1c"
      },
      "source": [
        "# Calling an inbuilt rosenbrock function \n",
        "\n",
        "rosen = so.rosen\n",
        "print (rosen)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<function rosen at 0x0000025BEE5E6B70>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDGW45D81lfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# declaring the rosenbrock\n",
        "\n",
        "def rosen_user(x):\n",
        "    return (1-x[0])**2+100*math.pow((x[1]- (x[0]**2)),2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mK7iHRN-1lfU",
        "colab_type": "code",
        "colab": {},
        "outputId": "b14a9e9e-6eaa-45d1-c768-27120dee2fc1"
      },
      "source": [
        "# minimzing the above functions with the help of different optimizers \n",
        "x=[0,2] # intial guess for the variable\n",
        "res = so.minimize(rosen_user,x,method='powell')\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   direc: array([[-7.90312985e-03, -1.47693647e-02],\n",
            "       [-9.73373762e-07, -2.01936048e-06]])\n",
            "     fun: 4.61495955505936e-28\n",
            " message: 'Optimization terminated successfully.'\n",
            "    nfev: 325\n",
            "     nit: 12\n",
            "  status: 0\n",
            " success: True\n",
            "       x: array([1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHet6FKU1lfZ",
        "colab_type": "text"
      },
      "source": [
        "##### The above result  consist of multiple arguments which are explained below\n",
        "#####  Objective value of the function is represent by \"fun\", Its value is almost equal to zero for this function. \"nfev\"  shows no of times objective function has evaluated.,  \"nit\" represents the no of the iteration taken by the optimizer., status represents status of termination. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMarHbDe1lfa",
        "colab_type": "code",
        "colab": {},
        "outputId": "b4e099de-0d7e-45c2-c737-9bf2cef6e872"
      },
      "source": [
        "\n",
        "x=[0,2]\n",
        "res = so.minimize(rosen,x,method='powell')\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   direc: array([[-7.90312985e-03, -1.47693647e-02],\n",
            "       [-9.73373762e-07, -2.01936048e-06]])\n",
            "     fun: 4.61495955505936e-28\n",
            " message: 'Optimization terminated successfully.'\n",
            "    nfev: 325\n",
            "     nit: 12\n",
            "  status: 0\n",
            " success: True\n",
            "       x: array([1., 1.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b6DYH7b1lff",
        "colab_type": "text"
      },
      "source": [
        "##### The above optmizer is not using gradient information which increase the no of iteration and late convergence to the function. To improve the convergence rate we will incorporate the gradient information to the optimizer.  Gradient can be incorporated in two ways\n",
        "##### 1. Calculating the gradient manually. \n",
        "##### 2. By default it will compute the value of the gradient. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0p-GXA11lfg",
        "colab_type": "code",
        "colab": {},
        "outputId": "c960f8f7-fc87-4c10-aba6-c93643f80e17"
      },
      "source": [
        "# Run with the default computation of the gradient \n",
        "res = so.minimize(rosen_user,x,method='BFGS')\n",
        "print(res)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "      fun: 2.1139082794782265e-11\n",
            " hess_inv: array([[0.48714168, 0.97239266],\n",
            "       [0.97239266, 1.94573529]])\n",
            "      jac: array([ 6.37411672e-06, -3.30079002e-06])\n",
            "  message: 'Optimization terminated successfully.'\n",
            "     nfev: 120\n",
            "      nit: 23\n",
            "     njev: 30\n",
            "   status: 0\n",
            "  success: True\n",
            "        x: array([0.99999541, 0.99999079])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKCfcAEE1lg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
